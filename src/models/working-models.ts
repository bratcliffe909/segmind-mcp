import { z } from 'zod';
import { ModelCategory, OutputType, ModelConfig } from './types.js';

// Based on actual testing, these are the confirmed working models
export const WORKING_MODELS: ModelConfig[] = [
  // Text-to-Image Models (4 working)
  {
    id: 'sdxl',
    name: 'Stable Diffusion XL',
    description: 'High-quality image generation with SDXL 1.0',
    category: ModelCategory.TEXT_TO_IMAGE,
    endpoint: '/sdxl1.0-txt2img',
    apiVersion: 'v1',
    outputType: OutputType.IMAGE,
    estimatedTime: 10,
    creditsPerUse: 0.3,
    parameters: z.object({
      prompt: z.string(),
      negative_prompt: z.string().optional(),
      img_width: z.number().min(256).max(2048).multipleOf(8).default(1024),
      img_height: z.number().min(256).max(2048).multipleOf(8).default(1024),
      samples: z.number().min(1).max(4).default(1),
      guidance_scale: z.number().min(1).max(20).default(7.5),
      num_inference_steps: z.number().min(1).max(100).default(25),
      seed: z.number().optional(),
      scheduler: z.string().optional().default('DDIM'),
      base64: z.boolean().optional().default(false),
    }),
    supportedFormats: ['png', 'jpeg', 'webp'],
    maxDimensions: { width: 2048, height: 2048 }
  },
  {
    id: 'sdxl-lightning',
    name: 'SDXL Lightning',
    description: 'Fast high-quality image generation with SDXL Lightning',
    category: ModelCategory.TEXT_TO_IMAGE,
    endpoint: '/sdxl1.0-newreality-lightning',
    apiVersion: 'v1',
    outputType: OutputType.IMAGE,
    estimatedTime: 5,
    creditsPerUse: 0.2,
    parameters: z.object({
      prompt: z.string(),
      negative_prompt: z.string().optional(),
      img_width: z.number().min(256).max(2048).multipleOf(8).default(512),
      img_height: z.number().min(256).max(2048).multipleOf(8).default(512),
      samples: z.number().min(1).max(4).default(1),
      guidance_scale: z.number().min(1).max(20).default(2),
      num_inference_steps: z.number().min(1).max(100).default(8),
      seed: z.number().optional(),
      base64: z.boolean().optional().default(false),
    }),
    supportedFormats: ['png', 'jpeg', 'webp'],
    maxDimensions: { width: 2048, height: 2048 }
  },
  {
    id: 'fooocus',
    name: 'Fooocus',
    description: 'Advanced image generation with Fooocus',
    category: ModelCategory.TEXT_TO_IMAGE,
    endpoint: '/fooocus',
    apiVersion: 'v1',
    outputType: OutputType.IMAGE,
    estimatedTime: 12,
    creditsPerUse: 0.4,
    parameters: z.object({
      prompt: z.string(),
      negative_prompt: z.string().optional().default(''),
      image_number: z.number().min(1).max(4).default(1),
      image_seed: z.number().optional().default(12345),
      sharpness: z.number().min(0).max(30).default(2),
      guidance_scale: z.number().min(1).max(30).default(7),
      base_model: z.string().optional().default('juggernautXL_version6Rundiffusion'),
      refiner_model: z.string().optional().default('None'),
      loras: z.string().optional().default('[]'),
    }),
    supportedFormats: ['png', 'jpeg'],
    maxDimensions: { width: 2048, height: 2048 }
  },
  {
    id: 'ssd-1b',
    name: 'SSD-1B',
    description: 'Efficient billion-parameter model for fast image generation',
    category: ModelCategory.TEXT_TO_IMAGE,
    endpoint: '/ssd-1b',
    apiVersion: 'v1',
    outputType: OutputType.IMAGE,
    estimatedTime: 8,
    creditsPerUse: 0.25,
    parameters: z.object({
      prompt: z.string(),
      negative_prompt: z.string().optional(),
      img_width: z.number().min(256).max(2048).multipleOf(8).default(512),
      img_height: z.number().min(256).max(2048).multipleOf(8).default(512),
      num_inference_steps: z.number().min(1).max(100).default(20),
      guidance_scale: z.number().min(1).max(20).default(7.5),
      seed: z.number().optional().default(12345),
    }),
    supportedFormats: ['png', 'jpeg'],
    maxDimensions: { width: 2048, height: 2048 }
  },

  // Image-to-Image Models (1 working)
  {
    id: 'sd15-img2img',
    name: 'SD 1.5 Image-to-Image',
    description: 'Transform existing images with Stable Diffusion 1.5',
    category: ModelCategory.IMAGE_TO_IMAGE,
    endpoint: '/sd1.5-img2img',
    apiVersion: 'v1',
    outputType: OutputType.IMAGE,
    estimatedTime: 8,
    creditsPerUse: 0.3,
    parameters: z.object({
      prompt: z.string(),
      negative_prompt: z.string().optional(),
      image: z.string().describe('Base64 encoded image'),
      samples: z.number().min(1).max(4).default(1),
      scheduler: z.string().optional().default('DDIM'),
      num_inference_steps: z.number().min(1).max(100).default(20),
      guidance_scale: z.number().min(1).max(20).default(7.5),
      strength: z.number().min(0).max(1).default(0.7),
      seed: z.number().optional(),
      base64: z.boolean().optional().default(false),
    }),
    supportedFormats: ['png', 'jpeg'],
    maxDimensions: { width: 1024, height: 1024 }
  },

  // Enhancement Models (2 working)
  {
    id: 'esrgan',
    name: 'ESRGAN',
    description: 'AI-powered image upscaling and enhancement',
    category: ModelCategory.IMAGE_ENHANCEMENT,
    endpoint: '/esrgan',
    apiVersion: 'v1',
    outputType: OutputType.IMAGE,
    estimatedTime: 5,
    creditsPerUse: 0.2,
    parameters: z.object({
      image: z.string().describe('Base64 encoded image'),
      scale: z.number().min(2).max(4).default(2),
      face_enhance: z.boolean().optional().default(false),
      base64: z.boolean().optional().default(false),
    }),
    supportedFormats: ['png', 'jpeg'],
  },
  {
    id: 'codeformer',
    name: 'CodeFormer',
    description: 'AI face restoration and enhancement',
    category: ModelCategory.IMAGE_ENHANCEMENT,
    endpoint: '/codeformer',
    apiVersion: 'v1',
    outputType: OutputType.IMAGE,
    estimatedTime: 5,
    creditsPerUse: 0.2,
    parameters: z.object({
      image: z.string().describe('Base64 encoded image'),
      fidelity: z.number().min(0).max(1).default(0.5),
      base64: z.boolean().optional().default(false),
    }),
    supportedFormats: ['png', 'jpeg'],
  },

  // Video Generation Models (2 working)
  {
    id: 'veo-3',
    name: 'Google Veo 3',
    description: 'Advanced text-to-video generation with realistic audio synthesis for cinematic content',
    category: ModelCategory.VIDEO_GENERATION,
    endpoint: '/veo-3',
    apiVersion: 'v1',
    outputType: OutputType.VIDEO,
    estimatedTime: 30,
    creditsPerUse: 2.0,
    parameters: z.object({
      prompt: z.string().min(1).max(2000).describe('Detailed description of the video content'),
      seed: z.number().int().optional().default(0).describe('Random seed for consistent outputs'),
    }),
    supportedFormats: ['mp4'],
  },
  {
    id: 'seedance-v1-lite',
    name: 'Seedance V1 Lite',
    description: 'Fast high-quality text-to-video generation with multi-shot capability',
    category: ModelCategory.VIDEO_GENERATION,
    endpoint: '/seedance-v1-lite-text-to-video',
    apiVersion: 'v1',
    outputType: OutputType.VIDEO,
    estimatedTime: 20,
    creditsPerUse: 0.45,
    parameters: z.object({
      prompt: z.string().min(1).max(1000).describe('Description of the video scene or animation'),
      duration: z.number().int().min(5).max(10).default(5).describe('Video duration in seconds'),
      aspect_ratio: z.enum(['16:9', '4:3', '1:1', '3:4', '9:16']).default('16:9'),
      resolution: z.enum(['480p', '720p']).default('720p'),
      seed: z.number().int().min(1).max(999999).optional(),
    }),
    supportedFormats: ['mp4'],
  },

  // Text-to-Speech Models (2 working)
  {
    id: 'dia-tts',
    name: 'Dia Text-to-Speech',
    description: 'Ultra-realistic multi-speaker dialogue with emotions and nonverbal cues',
    category: ModelCategory.SPECIALIZED_GENERATION,
    endpoint: '/dia',
    apiVersion: 'v1',
    outputType: OutputType.AUDIO,
    estimatedTime: 10,
    creditsPerUse: 0.15,
    parameters: z.object({
      text: z.string().min(1).max(5000).describe('Text with [S1], [S2] speaker tags and emotion cues'),
      top_p: z.number().min(0).max(1).default(0.95),
      cfg_scale: z.number().min(1).max(10).default(4),
      temperature: z.number().min(0.1).max(2).default(1.3),
      input_audio: z.string().optional().describe('Base64 audio for voice cloning'),
      speed_factor: z.number().min(0.5).max(2).default(0.94),
      max_new_tokens: z.number().min(512).max(4096).default(3072),
      cfg_filter_top_k: z.number().min(10).max(50).default(35),
    }),
    supportedFormats: ['mp3', 'wav'],
  },
  {
    id: 'orpheus-tts',
    name: 'Orpheus TTS 3B',
    description: 'Open-source TTS with emotion tags and natural conversational speech',
    category: ModelCategory.SPECIALIZED_GENERATION,
    endpoint: '/orpheus-3b-0.1',
    apiVersion: 'v1',
    outputType: OutputType.AUDIO,
    estimatedTime: 8,
    creditsPerUse: 0.1,
    parameters: z.object({
      text: z.string().min(1).max(2000).describe('Text with emotion tags like <laugh>, <sigh>'),
      voice: z.enum(['tara', 'dan', 'josh', 'emma']).default('dan'),
      top_p: z.number().min(0.1).max(1).default(0.95),
      temperature: z.number().min(0.1).max(1.5).default(0.6),
      max_new_tokens: z.number().min(100).max(2000).default(1200),
      repetition_penalty: z.number().min(1).max(2).default(1.1),
    }),
    supportedFormats: ['mp3', 'wav'],
  },

  // Music Generation Models (2 working)
  {
    id: 'lyria-2',
    name: 'Lyria 2',
    description: 'High-fidelity 48kHz stereo instrumental music generation from text',
    category: ModelCategory.SPECIALIZED_GENERATION,
    endpoint: '/lyria-2',
    apiVersion: 'v1',
    outputType: OutputType.AUDIO,
    estimatedTime: 25,
    creditsPerUse: 0.5,
    parameters: z.object({
      prompt: z.string().min(1).max(500).describe('Description of music theme and mood'),
      negative_prompt: z.string().optional().default('No loud drums, no vocals.'),
      seed: z.number().int().optional(),
    }),
    supportedFormats: ['mp3', 'wav'],
  },
  {
    id: 'minimax-music',
    name: 'Minimax Music-01',
    description: 'Generate up to 60 seconds of music with accompaniment and vocals',
    category: ModelCategory.SPECIALIZED_GENERATION,
    endpoint: '/minimax-music-01',
    apiVersion: 'v1',
    outputType: OutputType.AUDIO,
    estimatedTime: 40,
    creditsPerUse: 0.8,
    parameters: z.object({
      prompt: z.string().min(1).max(1000).describe('Music description or lyrics'),
      reference_audio: z.string().optional().describe('Base64 audio for voice reference'),
      instrumental_reference: z.string().optional().describe('Base64 instrumental reference'),
      duration: z.number().min(10).max(60).default(30),
    }),
    supportedFormats: ['mp3', 'wav'],
  },
];